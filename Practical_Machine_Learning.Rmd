---
title: "Practical Machine Learning"
author: "Massimo Malandra"
date: "18/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

### Overview:

This document contains and illustrates the analysis conducted for the final project of the *Practical Machine Learning* course offered by the Johns Hopkins University.
The goal of this work is to build a model who is able to predict the manner in which a certain person has done a certain physical exercise, based on a set of variables that register movements, using a series of special devices, as explained in the next paragraph.


### Background:

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement: a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 
A special thanks goes to the authors of this document who have been very generous in allowing their data to be used for this kind of assignment.

<br>

### Data loading and data cleaning:

For this project, two different sets of data have been provided: a training and a test set, that can be downloaded at the respective url indicated above. The first contains 19622 records, while the second only 20; both have 160 variables.

```{r}
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainingData <- read.csv(url(urlTrain), na.strings = c('','NA'))
testData <- read.csv(url(urlTest), na.strings = c('','NA'))
```

```{r eval = FALSE}
head(trainingData)
str(trainingData)  #19622 obs. of  160 variables

head(testData)
str(testData)  #20 obs. of  160 variables
```

Both the training and test files come with numerous NA values (we have already read the file in such a way that blanks are read as NA too): doing a quick check we see that 100 variables in both files have almost 98% of NAs, hence we will remove them from the analysis.
Moreover, having a quick look at the structure of the datasets, it is easy to see how the first 7 variables are not related in any way to body movements (for example: raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, etc.). We will remove these variables too, because they are not going to do anything else than adding noise to the model, not bringing any added value to the prediction of the manner in which the movement has been done.

```{r}
# Checking and removing NA in both train and test
naCountTrain <- sapply(trainingData, function(x) sum(is.na(x)))
table(naCountTrain)
trainingData <- trainingData[ , naCountTrain == 0]

naCountTest <- sapply(testData, function(x) sum(is.na(x)))
table(naCountTest)
testData <- testData[ , naCountTest == 0]
```

```{r eval = FALSE}
str(trainingData)
# 19622 obs. of  60 variables

str(testData)
# 20 obs. of  60 variables
```

```{r}
# Removing the first 7 variables who don't bring any added value to the purpose of the analysis:
trainingData <- trainingData[ , 8:60]
testData <- testData[ , 8:60]
```

```{r eval = FALSE}
dim(trainingData) # 19622 obs., 53 var.
dim(testData)  # 20 obs., 53 var.

colnames(trainingData) == colnames(testData) # all TRUE except the last one: OK
```

<br>

### Data partition:

Put aside the test set - who will be used only once the final model will be built and selected -, let's focus now on partitioning the training dataset in 2 different subsets: train (70%) and test (30%) sets again, based on the *classe* variable, who is the target variable of our analysis. The train set consists now of 13737 observations and 53 variables.

```{r message=FALSE}
# Partitioning the training set:
library(caret)
set.seed(12345)
inTrain <- createDataPartition(trainingData$classe, p=0.7, 
                               list=FALSE)
trainSet <- trainingData[inTrain, ]
testSet <- trainingData[-inTrain, ]
```

```{r eval = FALSE}
dim(trainSet)  #  13737 obs., 53 var.
dim(testSet)  #  5885 obs., 53 var.
```

<br>

### Prediction models:

Considering the type and the nature of the analytical problem we are facing and the corresponding datasets we have at our disposal, we can try to fit several different machine learning algorithms and evalutate all of them, before deciding which one to select and apply to the test set of 20 observations.

<br>

#### Decision Tree:

First of all, we will fit a *decision tree* model. The seed will be set at 12345 for reproducibility purposes. 
We will use the caret package and a *Repeated k-fold Cross Validation*, in which the process of splitting the data into 4 folds will be repeated 5 times. The final model accuracy is taken as the mean from the number of repeats.

```{r}
library(MLmetrics)
decTreeModel <- train(classe ~ ., method = "rpart", 
                   data = trainSet, tuneLength = 50, 
                   metric = "Accuracy",
                   trControl = trainControl(method = "repeatedcv",
                                            number = 4,
                                            repeats = 5))
```

```{r}
predDecTree <- predict(decTreeModel, testSet)
confusionMatrix(predDecTree, testSet$classe)
```

The figure below describes the trees with all of its nodes and splits.

```{r}
library(RColorBrewer)
library(rattle)
fancyRpartPlot(decTreeModel$finalModel)
```

The *accuracy* we achieve with this model is 0.8873, with a confidence interval of (0.879, 0.8953), hence an *out of sample error* (which is calculated as the 1 - accuracy) of 0.1127 or 11.27%. 
In order to better evaluate this result, we will fit other models, with the intent of reaching a better accuracy.

<br>

#### Random Forest:

As second option, we will try to fit a *Random Forest* model, with the default parameter of 500 as number of trees to grow. 
In order to avoid that our model would adjust too much to the twirks of our train set - and hence pick the random noise that it carries - we will use a *K-fold cross validation*, with 3 folds. This means that we will use 3 different pairs of train and test, calculating the corresponding errors. The average of these 3 errors will then represent a better estimate of the error we would get in an *out of sample* procedure. 


```{r message = FALSE}
# Random Forest model:
library(randomForest)
set.seed(12345)

controlRf = trainControl(method = "cv", number = 3, verboseIter = FALSE)
randForestModel <- randomForest(classe ~ ., data = trainSet,
                                trControl = controlRf)

predRandForest <- predict(randForestModel, testSet,
                          type = "class")
confusionMatrix(predRandForest, testSet$classe)
```

The *accuracy* given by this model is 0.9925, and an *out of sample error* of 0.0075 which is a surprisingly good result, so good that could lead to think of some sort of overfitting, even though the use of cross validation should prevent that.

<br>

#### Generalized Boosted Model:

Another model we can try to fit to our data in order to made the predictions on the test set is a Generalized Boosted Model.

```{r}
gbmModel <- train(classe ~ ., data = trainSet, method = "gbm", 
                 verbose = FALSE)
gbmPred <- predict(gbmModel, testSet)
confusionMatrix(gbmPred, testSet$classe)
```

The *accuracy* the model gives is 0.9573, hence an *out of sample error* of 0.0427.

The accuracy obtained using the **Random Forest** model is still the highest one and it will be then selected to be used to predict the type of movement of the 20 observations included in the test data.

<br>

### Scoring predictions:

The selected model is now used on the test data, in order to predict the 20 manners in which the movement has been done, indicated with letters from A to E.

```{r}
# Random Forest prediction:
randForPred <- predict(randForestModel, testData, type = "class")
randForPred
```
